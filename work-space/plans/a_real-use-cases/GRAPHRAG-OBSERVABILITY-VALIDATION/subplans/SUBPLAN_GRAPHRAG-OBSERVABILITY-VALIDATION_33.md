# SUBPLAN: Quality Metrics Validated

**Type**: SUBPLAN  
**Mother Plan**: PLAN_GRAPHRAG-OBSERVABILITY-VALIDATION.md  
**Plan**: GRAPHRAG-OBSERVABILITY-VALIDATION  
**Achievement**: 3.3  
**Created**: 2025-11-13  
**Status**: ðŸ“‹ Design Phase

---

## ðŸŽ¯ Objective

Validate that all quality metrics in the GraphRAG observability infrastructure calculate correctly with real pipeline data, verify API endpoints function properly, and ensure healthy range thresholds are appropriate for production use.

---

## ðŸ“¦ Deliverables

### Required Deliverables

1. **Metrics Validation Report** (`documentation/Quality-Metrics-Validation-Report.md`)

   - Comprehensive validation of all 23 quality metrics
   - Manual verification results for each metric
   - Accuracy assessment (calculated vs expected)
   - Issues found and resolutions

2. **Accuracy Verification Results** (`documentation/Quality-Metrics-Accuracy-Results.md`)

   - Stage-by-stage metric verification
   - Extraction metrics: entity_count_avg, confidence_avg, etc.
   - Resolution metrics: merge_rate, duplicate_reduction, etc.
   - Construction metrics: graph_density, average_degree, etc.
   - Detection metrics: modularity, community_count, etc.

3. **API Test Results** (`documentation/Quality-Metrics-API-Tests.md`)

   - Test results for all 3 API endpoints
   - Request/response examples
   - Performance measurements
   - Error handling validation

4. **Healthy Range Adjustments** (if needed)
   - Document any threshold adjustments required
   - Rationale for changes
   - Impact analysis

---

## ðŸŽ¯ Approach

### 5-Phase Sequential Execution

**Phase 1: Metrics Collection Verification** (45-60 min)

- Verify `graphrag_runs` collection contains metrics
- Verify `quality_metrics` collection contains time-series data
- Check trace_id linking across collections
- Validate metric calculation timestamps

**Phase 2: Extraction & Resolution Metrics Validation** (60-90 min)

- **Extraction Metrics** (7 metrics):
  - entity_count_avg, entity_count_total
  - confidence_avg, confidence_min, confidence_max
  - extraction_success_rate, extraction_duration_avg
- **Resolution Metrics** (6 metrics):
  - merge_rate, duplicate_reduction
  - entity_count_before, entity_count_after
  - resolution_success_rate, resolution_duration_avg
- Manual calculation and comparison with stored values

**Phase 3: Construction & Detection Metrics Validation** (60-90 min)

- **Construction Metrics** (5 metrics):
  - graph_density, average_degree
  - relationship_count, relationship_success_rate
  - construction_duration_avg
- **Detection Metrics** (5 metrics):
  - modularity, community_count
  - average_community_size, detection_success_rate
  - detection_duration_avg
- Manual calculation and comparison with stored values

**Phase 4: API Endpoint Testing** (30-45 min)

- Test `/api/quality/run?trace_id=<id>` endpoint
- Test `/api/quality/timeseries?stage=<stage>&metric=<metric>` endpoint
- Test `/api/quality/runs?limit=10` endpoint
- Validate response formats, error handling, performance

**Phase 5: Healthy Range Validation** (30-45 min)

- Review healthy ranges for all metrics
- Check which metrics are in/out of range
- Validate warning logging
- Adjust thresholds if needed based on real data

---

## ðŸ”„ Execution Strategy

### Single Sequential Execution

**Why Single Execution**: All phases build on each other and require access to the same pipeline data and API endpoints.

**Execution Flow**:

1. Phase 1: Verify data exists and is properly structured
2. Phase 2-3: Validate metric accuracy (can't proceed if Phase 1 fails)
3. Phase 4: Test API endpoints (requires validated metrics)
4. Phase 5: Adjust thresholds (requires understanding of all metrics)

**Prerequisites**:

- Achievement 2.2 complete (observability pipeline run with data)
- Trace ID from Achievement 2.2: `6088e6bd-e305-42d8-9210-e2d3f1dda035`
- MongoDB accessible with `validation_01` database
- API server running (if testing API endpoints)

---

## ðŸ§ª Tests

### Test 1: Metrics Collection Exists

**What**: Verify `graphrag_runs` and `quality_metrics` collections populated
**How**: Query MongoDB for trace_id documents
**Expected**: Both collections contain documents for trace_id
**Pass Criteria**: Document count > 0 in both collections

### Test 2: Trace ID Linking

**What**: Verify all metrics link to correct trace_id
**How**: Query all metrics, check trace_id field
**Expected**: All metrics have matching trace_id
**Pass Criteria**: 100% of metrics have correct trace_id

### Test 3: Extraction Metrics Accuracy

**What**: Manually verify extraction metrics calculations
**How**: Calculate from raw data, compare with stored values
**Expected**: Calculated values match stored values within 1% tolerance
**Pass Criteria**: All 7 extraction metrics within tolerance

### Test 4: Resolution Metrics Accuracy

**What**: Manually verify resolution metrics calculations
**How**: Calculate from raw data, compare with stored values
**Expected**: Calculated values match stored values within 1% tolerance
**Pass Criteria**: All 6 resolution metrics within tolerance

### Test 5: Construction Metrics Accuracy

**What**: Manually verify construction metrics calculations
**How**: Calculate from raw data, compare with stored values
**Expected**: Calculated values match stored values within 1% tolerance
**Pass Criteria**: All 5 construction metrics within tolerance

### Test 6: Detection Metrics Accuracy

**What**: Manually verify detection metrics calculations
**How**: Calculate from raw data, compare with stored values
**Expected**: Calculated values match stored values within 1% tolerance
**Pass Criteria**: All 5 detection metrics within tolerance

### Test 7: API Endpoint - Run Metrics

**What**: Test `/api/quality/run?trace_id=<id>` endpoint
**How**: curl request, validate response
**Expected**: Returns all metrics for trace_id in JSON format
**Pass Criteria**: HTTP 200, valid JSON, all metrics present

### Test 8: API Endpoint - Time Series

**What**: Test `/api/quality/timeseries` endpoint
**How**: curl request with stage and metric parameters
**Expected**: Returns time-series data for metric
**Pass Criteria**: HTTP 200, valid JSON, data points present

### Test 9: API Endpoint - Runs List

**What**: Test `/api/quality/runs?limit=10` endpoint
**How**: curl request, validate response
**Expected**: Returns list of recent runs
**Pass Criteria**: HTTP 200, valid JSON, runs list present

### Test 10: Healthy Range Validation

**What**: Verify healthy ranges are appropriate
**How**: Check which metrics are in/out of range, validate against real data
**Expected**: Ranges reflect realistic production values
**Pass Criteria**: < 30% of metrics out of range (indicates thresholds need adjustment)

---

## ðŸ“Š Expected Results

### Metrics Validation

**Extraction Stage** (7 metrics):

- entity_count_avg: ~7.46 (373 entities / 50 chunks)
- entity_count_total: 373
- confidence_avg: ~0.85-0.95
- confidence_min: â‰¥ 0.0
- confidence_max: â‰¤ 1.0
- extraction_success_rate: 100% (50/50 chunks)
- extraction_duration_avg: ~1-3 seconds per chunk

**Resolution Stage** (6 metrics):

- merge_rate: 0% (0 merges / 373 entities) - **Known Issue**
- duplicate_reduction: 0% (no duplicates removed)
- entity_count_before: 373
- entity_count_after: 373
- resolution_success_rate: 100%
- resolution_duration_avg: < 1 second

**Construction Stage** (5 metrics):

- graph_density: 0.0 (0 relationships) - **Known Issue**
- average_degree: 0.0 (no connections)
- relationship_count: 0 (all filtered)
- relationship_success_rate: 100% (processing succeeded)
- construction_duration_avg: ~1-2 seconds

**Detection Stage** (5 metrics):

- modularity: N/A (no communities)
- community_count: 0 - **Known Issue**
- average_community_size: N/A
- detection_success_rate: 100% (processing succeeded)
- detection_duration_avg: < 1 second

### API Endpoints

**Expected Responses**:

- All endpoints return HTTP 200
- JSON format valid and parsable
- Response times < 500ms for single run queries
- Error handling graceful (404 for missing trace_id)

### Healthy Ranges

**Expected Adjustments** (based on Achievement 2.2 data):

- merge_rate: Lower threshold to 0% (current data shows 0%)
- graph_density: Lower threshold to 0.0 (all relationships filtered)
- community_count: Lower threshold to 0 (no communities detected)

**Note**: These adjustments reflect **data quality issues** in the pipeline, not metric calculation errors.

---

## ðŸŽ“ Success Criteria

1. âœ… All 23 metrics verified for accuracy (within 1% tolerance)
2. âœ… All 3 API endpoints functional and tested
3. âœ… Healthy ranges validated and adjusted (if needed)
4. âœ… All 10 tests passed
5. âœ… All 4 deliverables created
6. âœ… No critical bugs found in metric calculations
7. âœ… Documentation complete with examples

---

## â±ï¸ Estimated Effort

**Total**: 3-4 hours

**Breakdown**:

- Phase 1: 45-60 min (collection verification)
- Phase 2: 60-90 min (extraction & resolution metrics)
- Phase 3: 60-90 min (construction & detection metrics)
- Phase 4: 30-45 min (API testing)
- Phase 5: 30-45 min (healthy range validation)

---

## ðŸ”— Dependencies

**Depends On**:

- Achievement 2.2 (Observability Pipeline Run) - **COMPLETE** âœ…
- Trace ID: `6088e6bd-e305-42d8-9210-e2d3f1dda035`
- MongoDB database: `validation_01`

**Blocks**:

- None (Achievement 3.3 is the last in Priority 3)

---

## ðŸ“‹ Notes

### Known Data Quality Issues

From Achievement 2.2, we know the pipeline has data quality issues:

- **0% merge rate** (no entities merged)
- **0 relationships** (all filtered out)
- **0 communities** (no graph structure)

These are **pipeline issues**, not metric calculation bugs. The metrics should correctly reflect these values.

### API Server

If API endpoints are not running, Phase 4 can be skipped or documented as "API not deployed". The core validation is metric accuracy (Phases 1-3, 5).

### Manual Calculation

Manual verification requires:

- Querying raw data from observability collections
- Calculating metrics using same formulas as code
- Comparing calculated vs stored values
- Documenting any discrepancies

---

**SUBPLAN Status**: ðŸ“‹ Ready for EXECUTION_TASK Creation  
**Next Step**: Create EXECUTION_TASK_GRAPHRAG-OBSERVABILITY-VALIDATION_33_01.md
